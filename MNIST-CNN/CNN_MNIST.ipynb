{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532697d2-d2d9-401d-93a7-32f115e64266",
   "metadata": {},
   "source": [
    "# **_MNIST Classification with Convolutional Neural Networks (CNN)_**\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will demonstrate how to build a Convolutional Neural Network (CNN) for classifying handwritten digits from the MNIST dataset. The CNN will be trained using TensorFlow and Keras, and we'll evaluate its performance on custom images drawn by the user. This notebook is divided into the following main steps:\n",
    "\n",
    "1. **Importing Libraries**: We will start by importing the necessary libraries required for the project.\n",
    "2. **Downloading and Preparing the Dataset**: We'll load the MNIST dataset using TensorFlow's built-in dataset API and preprocess the images.\n",
    "3. **Model Building**: We'll create a Convolutional Neural Network (CNN) using TensorFlow's Keras API.\n",
    "4. **Model Training**: The CNN will be trained using the prepared dataset.\n",
    "5. **Testing the Model**: Finally, we will test the trained model on new, custom images that we provide, and check its performance.\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "### **_1. Importing Necessary Libraries_**\n",
    "Before we begin the main process of building and training our Convolutional Neural Network (CNN) for digit classification on the MNIST dataset, we first need to import the essential libraries. Here's a breakdown of the libraries used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb441e1-e01b-414e-abde-4a02dd43f97e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:58:00.036096Z",
     "start_time": "2024-11-17T09:57:56.004319Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f61c1d-d70b-4df1-93f7-f662baf1a1f1",
   "metadata": {},
   "source": [
    "**_tensorflow:_** The primary library for building and training the CNN model.  \n",
    "**_cv2:_** Used for image processing tasks like loading, resizing, and manipulating images.  \n",
    "**_matplotlib.pyplot:_** Used for visualizing images and displaying results.  \n",
    "**_numpy:_** For numerical operations, particularly for handling image arrays.  \n",
    "**_os:_** For file system operations, such as checking directories or loading custom images.\n",
    "\n",
    "\n",
    "### **_2. Loading the MNIST Dataset_**\n",
    "We will now load the MNIST dataset, which contains images of handwritten digits (0-9). The dataset is available in TensorFlow and can be easily imported for training and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7eccc-aebd-42d8-812a-a292be6777ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:58:00.301914Z",
     "start_time": "2024-11-17T09:58:00.044552Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2fb10-f478-4b49-a78f-a82e4efe9bd4",
   "metadata": {},
   "source": [
    "The MNIST dataset contains grayscale images of handwritten digits (0-9). Each image is 28x28 pixels, represented by values ranging from 0 (black) to 255 (white).\n",
    "\n",
    "- **`x_train` and `x_test`**: These are the sets of images used for training and testing, respectively. `x_train` contains 60,000 images, and `x_test` contains 10,000. Each image in these arrays has a shape of `(28, 28)`.\n",
    "\n",
    "- **`y_train` and `y_test`**: These arrays contain the labels for each image, indicating the digit (0-9) that each image represents. The labels are stored as integers, so a `y_train` or `y_test` entry corresponds to the digit shown in the associated `x_train` or `x_test` image.\n",
    "\n",
    "*Note*: It's often helpful to normalize `x_train` and `x_test` by dividing by 255 to scale the pixel values to the range [0, 1], which can improve model training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d78d32-7d36-490e-bae2-716d4517da17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:58:00.763728Z",
     "start_time": "2024-11-17T09:58:00.318531Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc242f-626f-42cb-a94b-ad96e239ad2b",
   "metadata": {},
   "source": [
    "### **_3. Model building_**\n",
    "\n",
    "#### _Initializing the Model_\n",
    "\n",
    "To build our Convolutional Neural Network (CNN) model, we start by initializing a **Sequential** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa834a9-794c-4896-beae-5e23d8bd4758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T12:12:00.164206Z",
     "start_time": "2024-11-17T12:12:00.144595Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a14fd9-9e4d-4a02-8ec7-245e5dee63ac",
   "metadata": {},
   "source": [
    "This initializes a linear stack of layers, where each layer has exactly one input tensor and one output tensor. It’s ideal for a model that progresses layer by layer in a simple feed-forward manner.  \n",
    "Using a Sequential model is a straightforward way to build a CNN by adding layers one after the other, which we will do in the following steps.\n",
    "\n",
    "#### _Adding a Convolutional Layer_\n",
    "In a standard Artificial Neural Network (ANN), we feed the entire image directly into fully connected layers. However, with image data, it’s often more effective to **extract important features first** to simplify the learning process. That’s where **Convolutional Layers** come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d1bfd-396f-489d-811d-546fc5eb3310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T12:12:07.332011Z",
     "start_time": "2024-11-17T12:12:07.288939Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75012b3c-7313-490e-a717-6a7dd6151c92",
   "metadata": {},
   "source": [
    "This layer performs convolutions on the input image, using **32 filters** (also called kernels) that each scan across the image in **3x3 blocks**. Each filter learns to detect specific features (like edges or textures) as it scans through the image.  \n",
    "   - **`activation='relu'`**: The ReLU activation function introduces non-linearity, allowing the network to learn complex patterns.  \n",
    "   - **`input_shape=(28, 28, 1)`**: Specifies the shape of the input data. Here, `(28, 28, 1)` represents a 28x28 pixel grayscale image. The `1` indicates that the images are grayscale (single channel).  \n",
    "\n",
    "Adding this Conv2D layer allows the model to focus on important visual features first, before passing this feature-rich data to the rest of the network for classification.  \n",
    "\n",
    "\n",
    "#### _Adding a Pooling Layer_\n",
    "\n",
    "After applying **32 filters** in the first convolutional layer, we end up with 32 versions of the original image, each highlighting different features. This increases the number of parameters and the complexity of the data, which could slow down processing and make the model prone to overfitting.\n",
    "\n",
    "To manage this, we use **MaxPooling**, which helps in **reducing the dimensionality** of each feature map while retaining the most important information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f908e-1f91-41e9-bf42-bb297aad28dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T12:12:10.985572Z",
     "start_time": "2024-11-17T12:12:10.952902Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPooling2D((2, 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f64821-9184-4686-a088-66cc678bf1a0",
   "metadata": {},
   "source": [
    "  - **MaxPooling2D layer**: This layer downsamples each feature map by taking the maximum value in each **2x2 block**. By focusing on the strongest feature in each small region, we keep key information while discarding less relevant details.\n",
    "  - **Reducing Parameters**: Pooling reduces the size of each feature map, helping control the number of parameters the network must process, which also improves training efficiency.\n",
    "\n",
    "By adding MaxPooling, we simplify the feature maps, keeping only the strongest signals and reducing computational requirements, while preserving essential image features for later layers.\n",
    "\n",
    "\n",
    "#### _Adding a Second Convolutional Layer (Optional)_\n",
    "\n",
    "While adding a second convolutional layer is optional, it typically enhances the model’s ability to detect **more complex features** in the images. \n",
    "\n",
    "In the **first convolutional layer**, the network learns to identify basic patterns, such as **horizontal and vertical lines or simple edges**. However, with a second layer, the model can start to capture more intricate patterns like **curves and corners**, which are essential for understanding more complex shapes in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7dfe4b-b9d6-424e-a8a5-d73f32a7adca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T12:12:14.356190Z",
     "start_time": "2024-11-17T12:12:14.307440Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418f06d-aa6e-4b32-b033-61ff75d2216f",
   "metadata": {},
   "source": [
    "- This layer has **64 filters**, each scanning for slightly more advanced features than the first layer.\n",
    "\n",
    "### Transitioning Back to an ANN Workflow\n",
    "\n",
    "After extracting features using the convolutional and pooling layers, the rest of the process mirrors the steps followed in a standard **Artificial Neural Network (ANN)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f256e-12da-4ea8-8134-52627ecd5a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T12:12:23.177214Z",
     "start_time": "2024-11-17T12:12:22.001442Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "a = model.evaluate(x_test, y_test)\n",
    "print(a)\n",
    "\n",
    "model.save('./model/cnndigitrecog.h5')\n",
    "print(\"Trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20487d-de1d-41b3-bf6f-2fe331db4bb5",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Now  we can load the model and use it to predict handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d123e-15fa-4b57-a316-901581753422",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_recognizer = tf.keras.models.load_model('./model/cnndigitrecog.h5')\n",
    "# print(\"loaded from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f71a93-8a26-4f6d-b5a8-7e308cadb2b5",
   "metadata": {},
   "source": [
    "Now we upload custom images using open-cv, and reshape the images to (28, 28). If the image is a colored image, we have to convert it into gray-scale one. We create custom images as a tensorflow list, that has the same form of *x_test* or *x_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d61ca-e22f-44b1-bb46-a057242662fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_images = []\n",
    "for i in range(1, 10):\n",
    "    img = cv2.imread(f\"./data/{i}.jpg\")\n",
    "    img=cv2.resize(img, (28, 28))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.uint8(255-img)\n",
    "    custom_images.append(img)\n",
    "\n",
    "custom_images = np.array(custom_images)\n",
    "# print(np.shape(custom_images))\n",
    "\n",
    "\n",
    "custom_images = tf.constant(custom_images, dtype=tf.float32)\n",
    "pridictions = digit_recognizer.predict([custom_images])\n",
    "# print(len(pridictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa2a6d-1bae-4726-9373-83aa86fb0e5a",
   "metadata": {},
   "source": [
    "### Displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71a47a-b532-448d-8128-07dd10dce53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the axes array to easily iterate over it\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display each image in the corresponding subplot\n",
    "for i in range(9):\n",
    "    axes[i].imshow(custom_images[i], cmap='gray')\n",
    "    axes[i].axis('off')  # Hide the axis for better visualization\n",
    "    # axes[i].set_title(f\"True value: {i + 1}\")\n",
    "    # axes[i].set_xlabel(f\"Predicted: {pridictions[i]}\")\n",
    "    if i+1==np.argmax(pridictions[i]):\n",
    "        rang = \"green\"\n",
    "    else:\n",
    "        rang = \"red\"\n",
    "    axes[i].text(12.5, 30.5, f\"Predicted: {np.argmax(pridictions[i])}\", ha='center', va='center',  fontsize=15, color=rang)\n",
    "\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c931397",
   "metadata": {},
   "source": [
    "versio 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
