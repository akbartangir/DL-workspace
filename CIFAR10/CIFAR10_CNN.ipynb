{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VB0IL5_esto"
   },
   "source": [
    "1. Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsDAiVG4_v7a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duYIwcIPey02"
   },
   "source": [
    "2. Download cifar10 data, +format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD6_desjAA0m"
   },
   "outputs": [],
   "source": [
    "cf10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cf10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir5HftcUDrt9"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcrsQiVEfBNN"
   },
   "source": [
    "3. Make a model- Maxpool every conv, Batchnormalize every conv; dropout% disabled - affects accuracy negatively. Remove second dense if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia6DUCyb8h-n"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Initial Conv Block\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Second Conv Block\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Third Conv Block\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Dense Layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERJcYPJlff5u"
   },
   "source": [
    "4. Compile the model. 10 epochs for large dataset. 15+ has no accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC92_gGo80lO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Train with data augmentation\n",
    "history = model.fit(\n",
    "    data_augmentation(x_train),\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIiw37IC9ILw"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tHO9vCLAmC0"
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHyQaT1DftbN"
   },
   "source": [
    "5. Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BObhJXQDVGr"
   },
   "outputs": [],
   "source": [
    "eval = model.evaluate(x_test, y_test)\n",
    "# print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ9vSU5LfwQo"
   },
   "source": [
    "6. Visualisation (commented out because it gives errors, don't know why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l13ywv_X8_HS"
   },
   "outputs": [],
   "source": [
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess image\n",
    "    image = tf.keras.preprocessing.image.load_img(\n",
    "        image_path,\n",
    "        target_size=(32, 32)\n",
    "    )\n",
    "    image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    image_array = np.expand_dims(image_array, 0)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(image_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = float(np.max(predictions[0]))\n",
    "\n",
    "    # Display result\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image_array[0])\n",
    "    plt.title(f'Prediction: {predicted_class} ({confidence:.2%})')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WWYi0nhf98M"
   },
   "source": [
    "8. Predict user image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh7x82khFsRc"
   },
   "outputs": [],
   "source": [
    "image_path = 'deer1.jpg'\n",
    "predict_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUR3EPGFDA80"
   },
   "outputs": [],
   "source": [
    "# Save complete model (architecture + weights)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model.save(f'cifar10_cnn_model_{timestamp}.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model\n",
    "def load_saved_model(model_path):\n",
    "    \"\"\"Load a saved model from path\"\"\"\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Load the model\n",
    "# loaded_model = load_saved_model('cifar10_cnn_model_20240315_120000')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
